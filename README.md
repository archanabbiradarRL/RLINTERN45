# Tiny Transformer - Attention Assignments

This project implements a **Tiny Transformer** for character-level language modeling.

##  Features
- Attention and Multi-Head Attention from scratch
- Transformer blocks with/without positional encoding
- Training, evaluation, and text generation scripts
- Reproducible experiments with checkpoints

## ⚙️ Setup
```bash
git clone https://github.com/archanabbiradarRL/RLINTERN45.git
cd RLINTERN45
python -m venv .venv
source .venv/bin/activate   # On Linux/Mac
.venv\Scripts\activate    # On Windows
pip install -r requirements.txt
